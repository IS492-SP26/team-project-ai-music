1. Human AI Music Creation
Xu, Z., & Bryan-Kinns, N. (2025). DeformTune: A Deformable XAI Music Prototype for Non-Musicians. Proceedings of Explainable AI for the Arts Workshop (XAIxArts 2025).

Summary

This paper introduces DeformTune, a music generation system designed specifically for non-musicians. Instead of using text prompts or complex interfaces, it uses a deformable tactile surface that users press to control musical features like rhythm complexity and note density. The system connects physical pressure inputs to a latent space from a generative model and plays pre-generated MIDI outputs in real time. The researchers conducted a user study with 11 adults who had no formal musical training and found that while users enjoyed the system and found it expressive, they struggled to understand exactly how their actions mapped to the music output. The biggest takeaway was that users wanted clearer feedback and explanations about what the AI was doing.

Insights I Learned

> 1. Explainability in music AI is not just about showing technical details, but about helping users understand action-to-sound relationships.

> 2. There is a real tension between mystery and control. Users enjoy exploration, but still want predictable patterns they can learn from.

> 3. Physical affordances matter. Even the shape and feel of an interface changes how users expect it to behave.

Limitations / Risks

> The system used pre-generated MIDI combinations instead of real-time generation, which limits creative flexibility.

> The study only involved 11 participants, all within a similar age range, so findings might not generalize widely.

1 Idea This Inspires for Our Project

For BeatAI, we should make sure users understand why a beat or script was generated. Even simple explanations like “this chord progression creates tension” or “this rhythm increases energy” could make the system feel more educational and less random.

2. DeformTune
Newman, M., Morris, L., & Lee, J. H. (2023). Human-AI Music Creation: Understanding the Perceptions and Experiences of Music Creators for Ethical and Productive Collaboration. Proceedings of the 24th International Society for Music Information Retrieval Conference (ISMIR 2023).


Summary

This paper explores how professional music creators perceive and use AI tools in their composition process. The researchers conducted six in-depth case studies with creators across different musical contexts, including DJs, classical composers, and film composers. They found that creators are generally open to AI acting as a collaborator, especially for idea generation or repetitive tasks, but strongly resist AI replacing core creative decisions. Control and intention were major themes. Musicians were concerned about losing personality, authorship, and emotional connection if AI took over too much of the process. The paper proposes a Human-AI Creative Collaboration Model that shows how AI roles shift depending on personal context, social context, and creative goals.

Insights I Learned

> 1. Musicians are okay with AI helping, but not replacing intentional decision-making.

> 2. AI works best in early ideation or mechanical tasks, not final creative judgment.

> 3. Creative control is deeply tied to identity and authorship, not just output quality.

Limitations / Risks

> The study focused mostly on Western-trained professional musicians, so it doesn’t represent beginners or global contexts.

> It does not deeply explore how non-musicians might experience AI collaboration differently.

1 Idea This Inspires for Our Project

For BeatAI, we should design the system so that the final decisions always stay with the user. For example, offering multiple beat or script options instead of one final answer could preserve creative agency.


